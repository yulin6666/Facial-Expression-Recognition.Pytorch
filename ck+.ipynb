{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981, 48, 48)\n",
      "(981,)\n",
      "Save data finish!!!\n"
     ]
    }
   ],
   "source": [
    "!python preprocess_CK+.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "882 99\n",
      "882 99\n",
      "==> Building model..\n",
      "\n",
      "Epoch: 0\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.925 | Acc: 29.000% (260/882)        7/7 \n",
      "mainpro_CK+.py:141: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
      " [============================>.] | Loss: 1.950 | Acc: 12.000% (12/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 12.000\n",
      "\n",
      "Epoch: 1\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 1.363 | Acc: 48.000% (426/882)        7/7 \n",
      " [============================>.] | Loss: 2.103 | Acc: 6.000% (6/99)            20/20 \n",
      "\n",
      "Epoch: 2\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.968 | Acc: 61.000% (546/882)        7/7 \n",
      " [============================>.] | Loss: 2.571 | Acc: 6.000% (6/99)            20/20 \n",
      "\n",
      "Epoch: 3\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.692 | Acc: 71.000% (632/882)        7/7 \n",
      " [============================>.] | Loss: 2.509 | Acc: 18.000% (18/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 18.000\n",
      "\n",
      "Epoch: 4\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.535 | Acc: 80.000% (709/882)        7/7 \n",
      " [============================>.] | Loss: 1.492 | Acc: 51.000% (51/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 51.000\n",
      "\n",
      "Epoch: 5\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.441 | Acc: 83.000% (739/882)        7/7 \n",
      " [============================>.] | Loss: 1.412 | Acc: 34.000% (34/99)          20/20 \n",
      "\n",
      "Epoch: 6\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.405 | Acc: 85.000% (753/882)        7/7 \n",
      " [============================>.] | Loss: 1.127 | Acc: 68.000% (68/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 68.000\n",
      "\n",
      "Epoch: 7\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.366 | Acc: 86.000% (766/882)        7/7 \n",
      " [============================>.] | Loss: 0.789 | Acc: 70.000% (70/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 70.000\n",
      "\n",
      "Epoch: 8\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.356 | Acc: 86.000% (767/882)        7/7 \n",
      " [============================>.] | Loss: 0.554 | Acc: 83.000% (83/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 83.000\n",
      "\n",
      "Epoch: 9\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.252 | Acc: 91.000% (806/882)        7/7 \n",
      " [============================>.] | Loss: 0.734 | Acc: 79.000% (79/99)          20/20 \n",
      "\n",
      "Epoch: 10\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.247 | Acc: 91.000% (809/882)        7/7 \n",
      " [============================>.] | Loss: 0.375 | Acc: 80.000% (80/99)          20/20 \n",
      "\n",
      "Epoch: 11\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.183 | Acc: 93.000% (829/882)        7/7 \n",
      " [============================>.] | Loss: 0.599 | Acc: 78.000% (78/99)          20/20 \n",
      "\n",
      "Epoch: 12\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.138 | Acc: 96.000% (848/882)        7/7 \n",
      " [============================>.] | Loss: 0.459 | Acc: 78.000% (78/99)          20/20 \n",
      "\n",
      "Epoch: 13\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.137 | Acc: 95.000% (845/882)        7/7 \n",
      " [============================>.] | Loss: 0.562 | Acc: 85.000% (85/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 85.000\n",
      "\n",
      "Epoch: 14\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.114 | Acc: 96.000% (847/882)        7/7 \n",
      " [============================>.] | Loss: 0.547 | Acc: 78.000% (78/99)          20/20 \n",
      "\n",
      "Epoch: 15\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.101 | Acc: 96.000% (851/882)        7/7 \n",
      " [============================>.] | Loss: 0.718 | Acc: 75.000% (75/99)          20/20 \n",
      "\n",
      "Epoch: 16\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.066 | Acc: 98.000% (867/882)        7/7 \n",
      " [============================>.] | Loss: 0.479 | Acc: 74.000% (74/99)          20/20 \n",
      "\n",
      "Epoch: 17\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.076 | Acc: 97.000% (858/882)        7/7 \n",
      " [============================>.] | Loss: 0.310 | Acc: 87.000% (87/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 87.000\n",
      "\n",
      "Epoch: 18\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.058 | Acc: 98.000% (866/882)        7/7 \n",
      " [============================>.] | Loss: 0.785 | Acc: 76.000% (76/99)          20/20 \n",
      "\n",
      "Epoch: 19\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.082 | Acc: 97.000% (857/882)        7/7 \n",
      " [============================>.] | Loss: 0.257 | Acc: 91.000% (91/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 91.000\n",
      "\n",
      "Epoch: 20\n",
      "learning_rate: 0.01\n",
      " [=========================>....] | Loss: 0.079 | Acc: 97.000% (864/882)        7/7 \n",
      " [============================>.] | Loss: 1.023 | Acc: 65.000% (65/99)          20/20 \n",
      "\n",
      "Epoch: 21\n",
      "learning_rate: 0.008\n",
      " [=========================>....] | Loss: 0.035 | Acc: 98.000% (872/882)        7/7 \n",
      " [============================>.] | Loss: 0.257 | Acc: 94.000% (94/99)          20/20 \n",
      "Saving..\n",
      "best_Test_acc: 94.000\n",
      "\n",
      "Epoch: 22\n",
      "learning_rate: 0.006400000000000001\n",
      " [=========================>....] | Loss: 0.055 | Acc: 98.000% (868/882)        7/7 \n",
      " [============================>.] | Loss: 0.161 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 23\n",
      "learning_rate: 0.005120000000000001\n",
      " [=========================>....] | Loss: 0.042 | Acc: 98.000% (872/882)        7/7 \n",
      " [============================>.] | Loss: 0.207 | Acc: 94.000% (94/99)          20/20 \n",
      "\n",
      "Epoch: 24\n",
      "learning_rate: 0.004096000000000001\n",
      " [=========================>....] | Loss: 0.024 | Acc: 99.000% (877/882)        7/7 \n",
      " [============================>.] | Loss: 0.166 | Acc: 90.000% (90/99)          20/20 \n",
      "\n",
      "Epoch: 25\n",
      "learning_rate: 0.0032768000000000007\n",
      " [=========================>....] | Loss: 0.021 | Acc: 99.000% (876/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 91.000% (91/99)          20/20 \n",
      "\n",
      "Epoch: 26\n",
      "learning_rate: 0.002621440000000001\n",
      " [=========================>....] | Loss: 0.014 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.160 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 27\n",
      "learning_rate: 0.002097152000000001\n",
      " [=========================>....] | Loss: 0.021 | Acc: 99.000% (876/882)        7/7 \n",
      " [============================>.] | Loss: 0.173 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 28\n",
      "learning_rate: 0.001677721600000001\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.180 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 29\n",
      "learning_rate: 0.0013421772800000006\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.180 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 30\n",
      "learning_rate: 0.0010737418240000006\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.176 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 31\n",
      "learning_rate: 0.0008589934592000006\n",
      " [=========================>....] | Loss: 0.013 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.170 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 32\n",
      "learning_rate: 0.0006871947673600004\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.166 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 33\n",
      "learning_rate: 0.0005497558138880004\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.168 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 34\n",
      "learning_rate: 0.00043980465111040037\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.160 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 35\n",
      "learning_rate: 0.0003518437208883203\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.159 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 36\n",
      "learning_rate: 0.00028147497671065624\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.159 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 37\n",
      "learning_rate: 0.00022517998136852504\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 38\n",
      "learning_rate: 0.00018014398509482002\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 39\n",
      "learning_rate: 0.00014411518807585602\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 40\n",
      "learning_rate: 0.00011529215046068484\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 41\n",
      "learning_rate: 9.223372036854788e-05\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 42\n",
      "learning_rate: 7.37869762948383e-05\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 43\n",
      "learning_rate: 5.902958103587064e-05\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 44\n",
      "learning_rate: 4.722366482869652e-05\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 45\n",
      "learning_rate: 3.777893186295722e-05\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 46\n",
      "learning_rate: 3.0223145490365776e-05\n",
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 47\n",
      "learning_rate: 2.417851639229262e-05\n",
      " [=========================>....] | Loss: 0.006 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 48\n",
      "learning_rate: 1.9342813113834096e-05\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 49\n",
      "learning_rate: 1.547425049106728e-05\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 50\n",
      "learning_rate: 1.2379400392853824e-05\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 51\n",
      "learning_rate: 9.903520314283058e-06\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 52\n",
      "learning_rate: 7.922816251426448e-06\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.158 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 53\n",
      "learning_rate: 6.338253001141158e-06\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 54\n",
      "learning_rate: 5.0706024009129275e-06\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 55\n",
      "learning_rate: 4.056481920730342e-06\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 56\n",
      "learning_rate: 3.2451855365842735e-06\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 57\n",
      "learning_rate: 2.5961484292674196e-06\n",
      " [=========================>....] | Loss: 0.013 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 58\n",
      "learning_rate: 2.0769187434139356e-06\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 59\n",
      "learning_rate: 1.6615349947311485e-06\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 60\n",
      "learning_rate: 1.3292279957849189e-06\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 61\n",
      "learning_rate: 1.0633823966279351e-06\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.150 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 62\n",
      "learning_rate: 8.507059173023481e-07\n",
      " [=========================>....] | Loss: 0.006 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 63\n",
      "learning_rate: 6.805647338418786e-07\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 64\n",
      "learning_rate: 5.444517870735029e-07\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 65\n",
      "learning_rate: 4.3556142965880233e-07\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 66\n",
      "learning_rate: 3.484491437270419e-07\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 67\n",
      "learning_rate: 2.787593149816335e-07\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 68\n",
      "learning_rate: 2.2300745198530684e-07\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 69\n",
      "learning_rate: 1.784059615882455e-07\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (877/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 70\n",
      "learning_rate: 1.4272476927059639e-07\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 71\n",
      "learning_rate: 1.1417981541647711e-07\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 72\n",
      "learning_rate: 9.13438523331817e-08\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 73\n",
      "learning_rate: 7.307508186654536e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 74\n",
      "learning_rate: 5.846006549323629e-08\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.151 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 75\n",
      "learning_rate: 4.676805239458904e-08\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.151 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 76\n",
      "learning_rate: 3.741444191567123e-08\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 77\n",
      "learning_rate: 2.9931553532536985e-08\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 78\n",
      "learning_rate: 2.3945242826029592e-08\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 79\n",
      "learning_rate: 1.9156194260823674e-08\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 80\n",
      "learning_rate: 1.532495540865894e-08\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 81\n",
      "learning_rate: 1.2259964326927151e-08\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 82\n",
      "learning_rate: 9.807971461541723e-09\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 83\n",
      "learning_rate: 7.846377169233378e-09\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 84\n",
      "learning_rate: 6.277101735386703e-09\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 85\n",
      "learning_rate: 5.0216813883093625e-09\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 86\n",
      "learning_rate: 4.017345110647491e-09\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 87\n",
      "learning_rate: 3.2138760885179924e-09\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 88\n",
      "learning_rate: 2.5711008708143944e-09\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 89\n",
      "learning_rate: 2.0568806966515157e-09\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 90\n",
      "learning_rate: 1.6455045573212124e-09\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 91\n",
      "learning_rate: 1.31640364585697e-09\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 92\n",
      "learning_rate: 1.0531229166855762e-09\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 93\n",
      "learning_rate: 8.42498333348461e-10\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.159 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 94\n",
      "learning_rate: 6.739986666787687e-10\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.160 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 95\n",
      "learning_rate: 5.39198933343015e-10\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 96\n",
      "learning_rate: 4.3135914667441205e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 97\n",
      "learning_rate: 3.450873173395297e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 98\n",
      "learning_rate: 2.7606985387162373e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 99\n",
      "learning_rate: 2.2085588309729901e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 100\n",
      "learning_rate: 1.7668470647783923e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 101\n",
      "learning_rate: 1.413477651822714e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 102\n",
      "learning_rate: 1.130782121458171e-10\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 103\n",
      "learning_rate: 9.04625697166537e-11\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 104\n",
      "learning_rate: 7.237005577332295e-11\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 105\n",
      "learning_rate: 5.789604461865837e-11\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.151 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 106\n",
      "learning_rate: 4.63168356949267e-11\n",
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 107\n",
      "learning_rate: 3.7053468555941365e-11\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 108\n",
      "learning_rate: 2.964277484475309e-11\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 109\n",
      "learning_rate: 2.3714219875802474e-11\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 110\n",
      "learning_rate: 1.8971375900641982e-11\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 111\n",
      "learning_rate: 1.5177100720513584e-11\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 112\n",
      "learning_rate: 1.2141680576410869e-11\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 113\n",
      "learning_rate: 9.713344461128697e-12\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 114\n",
      "learning_rate: 7.770675568902958e-12\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 115\n",
      "learning_rate: 6.216540455122366e-12\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 116\n",
      "learning_rate: 4.973232364097893e-12\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 117\n",
      "learning_rate: 3.978585891278314e-12\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 118\n",
      "learning_rate: 3.182868713022652e-12\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 119\n",
      "learning_rate: 2.5462949704181215e-12\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 120\n",
      "learning_rate: 2.0370359763344977e-12\n",
      " [=========================>....] | Loss: 0.013 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 121\n",
      "learning_rate: 1.629628781067598e-12\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 122\n",
      "learning_rate: 1.3037030248540785e-12\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 123\n",
      "learning_rate: 1.0429624198832629e-12\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 124\n",
      "learning_rate: 8.343699359066104e-13\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 125\n",
      "learning_rate: 6.674959487252882e-13\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 126\n",
      "learning_rate: 5.339967589802307e-13\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 127\n",
      "learning_rate: 4.2719740718418454e-13\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 128\n",
      "learning_rate: 3.4175792574734765e-13\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 129\n",
      "learning_rate: 2.7340634059787813e-13\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 130\n",
      "learning_rate: 2.1872507247830254e-13\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 131\n",
      "learning_rate: 1.7498005798264204e-13\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 132\n",
      "learning_rate: 1.3998404638611363e-13\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 133\n",
      "learning_rate: 1.1198723710889092e-13\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 134\n",
      "learning_rate: 8.958978968711274e-14\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 135\n",
      "learning_rate: 7.167183174969019e-14\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 136\n",
      "learning_rate: 5.733746539975217e-14\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 137\n",
      "learning_rate: 4.5869972319801734e-14\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 138\n",
      "learning_rate: 3.669597785584139e-14\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 139\n",
      "learning_rate: 2.935678228467311e-14\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 140\n",
      "learning_rate: 2.3485425827738488e-14\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.158 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 141\n",
      "learning_rate: 1.878834066219079e-14\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.158 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 142\n",
      "learning_rate: 1.5030672529752636e-14\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 143\n",
      "learning_rate: 1.2024538023802108e-14\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 144\n",
      "learning_rate: 9.619630419041687e-15\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 145\n",
      "learning_rate: 7.69570433523335e-15\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 146\n",
      "learning_rate: 6.15656346818668e-15\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.150 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 147\n",
      "learning_rate: 4.925250774549344e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.151 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 148\n",
      "learning_rate: 3.940200619639476e-15\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 149\n",
      "learning_rate: 3.1521604957115808e-15\n",
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 150\n",
      "learning_rate: 2.521728396569265e-15\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 151\n",
      "learning_rate: 2.017382717255412e-15\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 152\n",
      "learning_rate: 1.6139061738043298e-15\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 153\n",
      "learning_rate: 1.2911249390434638e-15\n",
      " [=========================>....] | Loss: 0.007 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 154\n",
      "learning_rate: 1.0328999512347711e-15\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 155\n",
      "learning_rate: 8.263199609878169e-16\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 156\n",
      "learning_rate: 6.610559687902537e-16\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 157\n",
      "learning_rate: 5.288447750322029e-16\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 158\n",
      "learning_rate: 4.2307582002576235e-16\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 159\n",
      "learning_rate: 3.384606560206099e-16\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 160\n",
      "learning_rate: 2.7076852481648796e-16\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 161\n",
      "learning_rate: 2.1661481985319035e-16\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 162\n",
      "learning_rate: 1.732918558825523e-16\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 163\n",
      "learning_rate: 1.3863348470604184e-16\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.151 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 164\n",
      "learning_rate: 1.1090678776483348e-16\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 165\n",
      "learning_rate: 8.87254302118668e-17\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 166\n",
      "learning_rate: 7.098034416949344e-17\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 167\n",
      "learning_rate: 5.678427533559476e-17\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 168\n",
      "learning_rate: 4.5427420268475807e-17\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 169\n",
      "learning_rate: 3.6341936214780644e-17\n",
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 170\n",
      "learning_rate: 2.907354897182452e-17\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 171\n",
      "learning_rate: 2.3258839177459616e-17\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 172\n",
      "learning_rate: 1.8607071341967693e-17\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 173\n",
      "learning_rate: 1.4885657073574156e-17\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 174\n",
      "learning_rate: 1.1908525658859325e-17\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 175\n",
      "learning_rate: 9.52682052708746e-18\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 176\n",
      "learning_rate: 7.62145642166997e-18\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 177\n",
      "learning_rate: 6.0971651373359754e-18\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 178\n",
      "learning_rate: 4.877732109868781e-18\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 179\n",
      "learning_rate: 3.902185687895025e-18\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 180\n",
      "learning_rate: 3.12174855031602e-18\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 181\n",
      "learning_rate: 2.497398840252816e-18\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 182\n",
      "learning_rate: 1.997919072202253e-18\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 183\n",
      "learning_rate: 1.5983352577618025e-18\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 184\n",
      "learning_rate: 1.278668206209442e-18\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 185\n",
      "learning_rate: 1.0229345649675538e-18\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 186\n",
      "learning_rate: 8.18347651974043e-19\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 187\n",
      "learning_rate: 6.546781215792345e-19\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.161 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 188\n",
      "learning_rate: 5.237424972633876e-19\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 189\n",
      "learning_rate: 4.189939978107101e-19\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 190\n",
      "learning_rate: 3.3519519824856807e-19\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 191\n",
      "learning_rate: 2.681561585988545e-19\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 192\n",
      "learning_rate: 2.145249268790836e-19\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 193\n",
      "learning_rate: 1.7161994150326688e-19\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.151 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 194\n",
      "learning_rate: 1.3729595320261352e-19\n",
      " [=========================>....] | Loss: 0.015 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 195\n",
      "learning_rate: 1.098367625620908e-19\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.150 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 196\n",
      "learning_rate: 8.786941004967267e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 197\n",
      "learning_rate: 7.029552803973813e-20\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 198\n",
      "learning_rate: 5.623642243179051e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 199\n",
      "learning_rate: 4.498913794543241e-20\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 200\n",
      "learning_rate: 3.5991310356345934e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 201\n",
      "learning_rate: 2.8793048285076746e-20\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 202\n",
      "learning_rate: 2.30344386280614e-20\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 203\n",
      "learning_rate: 1.8427550902449122e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 204\n",
      "learning_rate: 1.4742040721959298e-20\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 205\n",
      "learning_rate: 1.1793632577567439e-20\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 206\n",
      "learning_rate: 9.43490606205395e-21\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 207\n",
      "learning_rate: 7.547924849643161e-21\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 208\n",
      "learning_rate: 6.0383398797145295e-21\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 209\n",
      "learning_rate: 4.8306719037716234e-21\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 210\n",
      "learning_rate: 3.864537523017299e-21\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 211\n",
      "learning_rate: 3.0916300184138396e-21\n",
      " [=========================>....] | Loss: 0.013 | Acc: 99.000% (877/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 212\n",
      "learning_rate: 2.4733040147310717e-21\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 213\n",
      "learning_rate: 1.9786432117848575e-21\n",
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 214\n",
      "learning_rate: 1.5829145694278862e-21\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.158 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 215\n",
      "learning_rate: 1.266331655542309e-21\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 216\n",
      "learning_rate: 1.0130653244338472e-21\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 217\n",
      "learning_rate: 8.104522595470779e-22\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 218\n",
      "learning_rate: 6.483618076376623e-22\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 219\n",
      "learning_rate: 5.186894461101298e-22\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 220\n",
      "learning_rate: 4.149515568881039e-22\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 221\n",
      "learning_rate: 3.3196124551048316e-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [=========================>....] | Loss: 0.008 | Acc: 100.000% (882/882)       7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 222\n",
      "learning_rate: 2.6556899640838656e-22\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 223\n",
      "learning_rate: 2.1245519712670924e-22\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 224\n",
      "learning_rate: 1.699641577013674e-22\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 225\n",
      "learning_rate: 1.3597132616109392e-22\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 226\n",
      "learning_rate: 1.0877706092887513e-22\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 227\n",
      "learning_rate: 8.702164874310012e-23\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 228\n",
      "learning_rate: 6.96173189944801e-23\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.155 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 229\n",
      "learning_rate: 5.569385519558409e-23\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 230\n",
      "learning_rate: 4.455508415646727e-23\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 231\n",
      "learning_rate: 3.5644067325173816e-23\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 232\n",
      "learning_rate: 2.851525386013906e-23\n",
      " [=========================>....] | Loss: 0.013 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 233\n",
      "learning_rate: 2.2812203088111247e-23\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 234\n",
      "learning_rate: 1.8249762470489e-23\n",
      " [=========================>....] | Loss: 0.012 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 235\n",
      "learning_rate: 1.45998099763912e-23\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 236\n",
      "learning_rate: 1.167984798111296e-23\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 237\n",
      "learning_rate: 9.343878384890368e-24\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 238\n",
      "learning_rate: 7.475102707912296e-24\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 239\n",
      "learning_rate: 5.9800821663298366e-24\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 240\n",
      "learning_rate: 4.784065733063869e-24\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.154 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 241\n",
      "learning_rate: 3.827252586451096e-24\n",
      " [=========================>....] | Loss: 0.008 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.157 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 242\n",
      "learning_rate: 3.061802069160877e-24\n",
      " [=========================>....] | Loss: 0.007 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.156 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 243\n",
      "learning_rate: 2.4494416553287016e-24\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 244\n",
      "learning_rate: 1.9595533242629614e-24\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (881/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 245\n",
      "learning_rate: 1.5676426594103693e-24\n",
      " [=========================>....] | Loss: 0.013 | Acc: 99.000% (880/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 246\n",
      "learning_rate: 1.2541141275282953e-24\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (878/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 247\n",
      "learning_rate: 1.0032913020226365e-24\n",
      " [=========================>....] | Loss: 0.009 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 248\n",
      "learning_rate: 8.026330416181091e-25\n",
      " [=========================>....] | Loss: 0.010 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.153 | Acc: 93.000% (93/99)          20/20 \n",
      "\n",
      "Epoch: 249\n",
      "learning_rate: 6.421064332944874e-25\n",
      " [=========================>....] | Loss: 0.011 | Acc: 99.000% (879/882)        7/7 \n",
      " [============================>.] | Loss: 0.152 | Acc: 93.000% (93/99)          20/20 \n",
      "best_Test_acc: 94.000\n",
      "best_Test_acc_epoch: 21\n"
     ]
    }
   ],
   "source": [
    "!python mainpro_CK+.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin      5.899\n",
      "origin      0.868\n",
      "origin     -3.237\n",
      "origin     -2.626\n",
      "origin      2.764\n",
      "origin     -0.591\n",
      "origin      1.633\n",
      "     0.938\n",
      "     0.006\n",
      "     0.000\n",
      "     0.000\n",
      "     0.041\n",
      "     0.001\n",
      "     0.013\n",
      "The Expression is Angry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    }
   ],
   "source": [
    "#验证模型正确性\n",
    "\"\"\"\n",
    "visualize results for test image\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "raw_img = io.imread('images/anger.png')\n",
    "img = raw_img[:, :, np.newaxis]\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "net = VGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('CK+_VGG19/1/', 'Test_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1, c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "outputs = net(inputs)\n",
    "for i in range(7):\n",
    "  print('origin %10.3f' % outputs[0][i])\n",
    "\n",
    "score = F.softmax(outputs,1)\n",
    "max = score[0][0]\n",
    "maxindex = 0\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "  if(score[0][i] > max):\n",
    "        max = score[0][i]\n",
    "        maxindex = i\n",
    "\n",
    "\n",
    "        print(\"The Expression is %s\" %str(class_names[maxindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (1.5.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.17.0)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.12.0)\r\n",
      "Requirement already satisfied: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4)\r\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.9.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.1)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (41.0.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%data : Float(1, 3, 48, 48),\n",
      "      %1 : Float(64, 3, 3, 3),\n",
      "      %2 : Float(64),\n",
      "      %3 : Float(64),\n",
      "      %4 : Float(64),\n",
      "      %5 : Float(64),\n",
      "      %6 : Float(64),\n",
      "      %7 : Long(),\n",
      "      %8 : Float(64, 64, 3, 3),\n",
      "      %9 : Float(64),\n",
      "      %10 : Float(64),\n",
      "      %11 : Float(64),\n",
      "      %12 : Float(64),\n",
      "      %13 : Float(64),\n",
      "      %14 : Long(),\n",
      "      %15 : Float(128, 64, 3, 3),\n",
      "      %16 : Float(128),\n",
      "      %17 : Float(128),\n",
      "      %18 : Float(128),\n",
      "      %19 : Float(128),\n",
      "      %20 : Float(128),\n",
      "      %21 : Long(),\n",
      "      %22 : Float(128, 128, 3, 3),\n",
      "      %23 : Float(128),\n",
      "      %24 : Float(128),\n",
      "      %25 : Float(128),\n",
      "      %26 : Float(128),\n",
      "      %27 : Float(128),\n",
      "      %28 : Long(),\n",
      "      %29 : Float(256, 128, 3, 3),\n",
      "      %30 : Float(256),\n",
      "      %31 : Float(256),\n",
      "      %32 : Float(256),\n",
      "      %33 : Float(256),\n",
      "      %34 : Float(256),\n",
      "      %35 : Long(),\n",
      "      %36 : Float(256, 256, 3, 3),\n",
      "      %37 : Float(256),\n",
      "      %38 : Float(256),\n",
      "      %39 : Float(256),\n",
      "      %40 : Float(256),\n",
      "      %41 : Float(256),\n",
      "      %42 : Long(),\n",
      "      %43 : Float(256, 256, 3, 3),\n",
      "      %44 : Float(256),\n",
      "      %45 : Float(256),\n",
      "      %46 : Float(256),\n",
      "      %47 : Float(256),\n",
      "      %48 : Float(256),\n",
      "      %49 : Long(),\n",
      "      %50 : Float(256, 256, 3, 3),\n",
      "      %51 : Float(256),\n",
      "      %52 : Float(256),\n",
      "      %53 : Float(256),\n",
      "      %54 : Float(256),\n",
      "      %55 : Float(256),\n",
      "      %56 : Long(),\n",
      "      %57 : Float(512, 256, 3, 3),\n",
      "      %58 : Float(512),\n",
      "      %59 : Float(512),\n",
      "      %60 : Float(512),\n",
      "      %61 : Float(512),\n",
      "      %62 : Float(512),\n",
      "      %63 : Long(),\n",
      "      %64 : Float(512, 512, 3, 3),\n",
      "      %65 : Float(512),\n",
      "      %66 : Float(512),\n",
      "      %67 : Float(512),\n",
      "      %68 : Float(512),\n",
      "      %69 : Float(512),\n",
      "      %70 : Long(),\n",
      "      %71 : Float(512, 512, 3, 3),\n",
      "      %72 : Float(512),\n",
      "      %73 : Float(512),\n",
      "      %74 : Float(512),\n",
      "      %75 : Float(512),\n",
      "      %76 : Float(512),\n",
      "      %77 : Long(),\n",
      "      %78 : Float(512, 512, 3, 3),\n",
      "      %79 : Float(512),\n",
      "      %80 : Float(512),\n",
      "      %81 : Float(512),\n",
      "      %82 : Float(512),\n",
      "      %83 : Float(512),\n",
      "      %84 : Long(),\n",
      "      %85 : Float(512, 512, 3, 3),\n",
      "      %86 : Float(512),\n",
      "      %87 : Float(512),\n",
      "      %88 : Float(512),\n",
      "      %89 : Float(512),\n",
      "      %90 : Float(512),\n",
      "      %91 : Long(),\n",
      "      %92 : Float(512, 512, 3, 3),\n",
      "      %93 : Float(512),\n",
      "      %94 : Float(512),\n",
      "      %95 : Float(512),\n",
      "      %96 : Float(512),\n",
      "      %97 : Float(512),\n",
      "      %98 : Long(),\n",
      "      %99 : Float(512, 512, 3, 3),\n",
      "      %100 : Float(512),\n",
      "      %101 : Float(512),\n",
      "      %102 : Float(512),\n",
      "      %103 : Float(512),\n",
      "      %104 : Float(512),\n",
      "      %105 : Long(),\n",
      "      %106 : Float(512, 512, 3, 3),\n",
      "      %107 : Float(512),\n",
      "      %108 : Float(512),\n",
      "      %109 : Float(512),\n",
      "      %110 : Float(512),\n",
      "      %111 : Float(512),\n",
      "      %112 : Long(),\n",
      "      %113 : Float(7, 512),\n",
      "      %114 : Float(7)):\n",
      "  %115 : Float(1, 64, 48, 48) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%data, %1, %2), scope: VGG/Sequential[features]/Conv2d[0] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %116 : Float(1, 64, 48, 48) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%115, %3, %4, %5, %6), scope: VGG/Sequential[features]/BatchNorm2d[1] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %117 : Float(1, 64, 48, 48) = onnx::Relu(%116), scope: VGG/Sequential[features]/ReLU[2] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %118 : Float(1, 64, 48, 48) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%117, %8, %9), scope: VGG/Sequential[features]/Conv2d[3] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %119 : Float(1, 64, 48, 48) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%118, %10, %11, %12, %13), scope: VGG/Sequential[features]/BatchNorm2d[4] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %120 : Float(1, 64, 48, 48) = onnx::Relu(%119), scope: VGG/Sequential[features]/ReLU[5] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %121 : Float(1, 64, 24, 24) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%120), scope: VGG/Sequential[features]/MaxPool2d[6] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:487:0\n",
      "  %122 : Float(1, 128, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%121, %15, %16), scope: VGG/Sequential[features]/Conv2d[7] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %123 : Float(1, 128, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%122, %17, %18, %19, %20), scope: VGG/Sequential[features]/BatchNorm2d[8] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %124 : Float(1, 128, 24, 24) = onnx::Relu(%123), scope: VGG/Sequential[features]/ReLU[9] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %125 : Float(1, 128, 24, 24) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%124, %22, %23), scope: VGG/Sequential[features]/Conv2d[10] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %126 : Float(1, 128, 24, 24) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%125, %24, %25, %26, %27), scope: VGG/Sequential[features]/BatchNorm2d[11] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %127 : Float(1, 128, 24, 24) = onnx::Relu(%126), scope: VGG/Sequential[features]/ReLU[12] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %128 : Float(1, 128, 12, 12) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%127), scope: VGG/Sequential[features]/MaxPool2d[13] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:487:0\n",
      "  %129 : Float(1, 256, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%128, %29, %30), scope: VGG/Sequential[features]/Conv2d[14] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %130 : Float(1, 256, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%129, %31, %32, %33, %34), scope: VGG/Sequential[features]/BatchNorm2d[15] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %131 : Float(1, 256, 12, 12) = onnx::Relu(%130), scope: VGG/Sequential[features]/ReLU[16] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %132 : Float(1, 256, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%131, %36, %37), scope: VGG/Sequential[features]/Conv2d[17] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %133 : Float(1, 256, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%132, %38, %39, %40, %41), scope: VGG/Sequential[features]/BatchNorm2d[18] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %134 : Float(1, 256, 12, 12) = onnx::Relu(%133), scope: VGG/Sequential[features]/ReLU[19] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %135 : Float(1, 256, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%134, %43, %44), scope: VGG/Sequential[features]/Conv2d[20] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %136 : Float(1, 256, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%135, %45, %46, %47, %48), scope: VGG/Sequential[features]/BatchNorm2d[21] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %137 : Float(1, 256, 12, 12) = onnx::Relu(%136), scope: VGG/Sequential[features]/ReLU[22] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %138 : Float(1, 256, 12, 12) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%137, %50, %51), scope: VGG/Sequential[features]/Conv2d[23] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %139 : Float(1, 256, 12, 12) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%138, %52, %53, %54, %55), scope: VGG/Sequential[features]/BatchNorm2d[24] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %140 : Float(1, 256, 12, 12) = onnx::Relu(%139), scope: VGG/Sequential[features]/ReLU[25] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %141 : Float(1, 256, 6, 6) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%140), scope: VGG/Sequential[features]/MaxPool2d[26] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:487:0\n",
      "  %142 : Float(1, 512, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%141, %57, %58), scope: VGG/Sequential[features]/Conv2d[27] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %143 : Float(1, 512, 6, 6) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%142, %59, %60, %61, %62), scope: VGG/Sequential[features]/BatchNorm2d[28] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %144 : Float(1, 512, 6, 6) = onnx::Relu(%143), scope: VGG/Sequential[features]/ReLU[29] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %145 : Float(1, 512, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%144, %64, %65), scope: VGG/Sequential[features]/Conv2d[30] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %146 : Float(1, 512, 6, 6) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%145, %66, %67, %68, %69), scope: VGG/Sequential[features]/BatchNorm2d[31] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %147 : Float(1, 512, 6, 6) = onnx::Relu(%146), scope: VGG/Sequential[features]/ReLU[32] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %148 : Float(1, 512, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%147, %71, %72), scope: VGG/Sequential[features]/Conv2d[33] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %149 : Float(1, 512, 6, 6) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%148, %73, %74, %75, %76), scope: VGG/Sequential[features]/BatchNorm2d[34] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %150 : Float(1, 512, 6, 6) = onnx::Relu(%149), scope: VGG/Sequential[features]/ReLU[35] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %151 : Float(1, 512, 6, 6) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%150, %78, %79), scope: VGG/Sequential[features]/Conv2d[36] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %152 : Float(1, 512, 6, 6) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%151, %80, %81, %82, %83), scope: VGG/Sequential[features]/BatchNorm2d[37] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %153 : Float(1, 512, 6, 6) = onnx::Relu(%152), scope: VGG/Sequential[features]/ReLU[38] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %154 : Float(1, 512, 3, 3) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%153), scope: VGG/Sequential[features]/MaxPool2d[39] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:487:0\n",
      "  %155 : Float(1, 512, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%154, %85, %86), scope: VGG/Sequential[features]/Conv2d[40] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %156 : Float(1, 512, 3, 3) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%155, %87, %88, %89, %90), scope: VGG/Sequential[features]/BatchNorm2d[41] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %157 : Float(1, 512, 3, 3) = onnx::Relu(%156), scope: VGG/Sequential[features]/ReLU[42] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %158 : Float(1, 512, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%157, %92, %93), scope: VGG/Sequential[features]/Conv2d[43] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %159 : Float(1, 512, 3, 3) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%158, %94, %95, %96, %97), scope: VGG/Sequential[features]/BatchNorm2d[44] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %160 : Float(1, 512, 3, 3) = onnx::Relu(%159), scope: VGG/Sequential[features]/ReLU[45] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %161 : Float(1, 512, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%160, %99, %100), scope: VGG/Sequential[features]/Conv2d[46] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %162 : Float(1, 512, 3, 3) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%161, %101, %102, %103, %104), scope: VGG/Sequential[features]/BatchNorm2d[47] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %163 : Float(1, 512, 3, 3) = onnx::Relu(%162), scope: VGG/Sequential[features]/ReLU[48] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %164 : Float(1, 512, 3, 3) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%163, %106, %107), scope: VGG/Sequential[features]/Conv2d[49] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py:340:0\n",
      "  %165 : Float(1, 512, 3, 3) = onnx::BatchNormalization[epsilon=1e-05, momentum=0.9](%164, %108, %109, %110, %111), scope: VGG/Sequential[features]/BatchNorm2d[50] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1656:0\n",
      "  %166 : Float(1, 512, 3, 3) = onnx::Relu(%165), scope: VGG/Sequential[features]/ReLU[51] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:911:0\n",
      "  %167 : Float(1, 512, 1, 1) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%166), scope: VGG/Sequential[features]/MaxPool2d[52] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:487:0\n",
      "  %168 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0](%167), scope: VGG/Sequential[features]/AvgPool2d[53]\n",
      "  %169 : Float(1, 512, 1, 1) = onnx::AveragePool[kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%168), scope: VGG/Sequential[features]/AvgPool2d[53] # /usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py:551:0\n",
      "  %170 : Long() = onnx::Constant[value={0}](), scope: VGG\n",
      "  %171 : Tensor = onnx::Shape(%169), scope: VGG\n",
      "  %172 : Long() = onnx::Gather[axis=0](%171, %170), scope: VGG # /storage/Facial-Expression-Recognition.Pytorch/models/vgg.py:24:0\n",
      "  %173 : Long() = onnx::Constant[value={-1}](), scope: VGG\n",
      "  %174 : Tensor = onnx::Unsqueeze[axes=[0]](%172)\n",
      "  %175 : Tensor = onnx::Unsqueeze[axes=[0]](%173)\n",
      "  %176 : Tensor = onnx::Concat[axis=0](%174, %175)\n",
      "  %177 : Float(1, 512) = onnx::Reshape(%169, %176), scope: VGG # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:806:0\n",
      "  %outTensor : Float(1, 7) = onnx::Gemm[alpha=1, beta=1, transB=1](%177, %113, %114), scope: VGG/Linear[classifier] # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1369:0\n",
      "  return (%outTensor)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#转成onnx模型\n",
    "!pip install onnx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "img = io.imread('images/anger.png')\n",
    "img = img[:, :, np.newaxis]\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "#导入模型，用训练模式\n",
    "net = VGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('CK+_VGG19/1/', 'Test_model.t7'))\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "net.cuda()\n",
    "net.train(False)\n",
    "\n",
    "#模拟input\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "inputs = inputs.cuda()\n",
    "inputs = Variable(inputs, volatile=True)\n",
    "#导出模型\n",
    "torch_out = torch.onnx._export(net,  # model being run\n",
    "                               inputs,  # model input (or a tuple for multiple inputs)\n",
    "                               \"CK+_VGG19_privateTest.onnx\",  # where to save the model\n",
    "                               verbose=True,\n",
    "                               input_names=['data'], \n",
    "                               output_names=['outTensor'], \n",
    "                               export_params=True, \n",
    "                               training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.898687    0.8682228  -3.2367802  -2.6263356   2.7641838  -0.59135896\n",
      "   1.6326844 ]]\n",
      "     0.938\n",
      "     0.006\n",
      "     0.000\n",
      "     0.000\n",
      "     0.041\n",
      "     0.001\n",
      "     0.013\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "# 验证onnx模型\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"CK+_VGG19_privateTest.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "\n",
    "img = io.imread('images/anger.png')\n",
    "img = img[:, :, np.newaxis]\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "max = score[0][0]\n",
    "maxindex = 0\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "  if(score[0][i] > max):\n",
    "        max = score[0][i]\n",
    "        maxindex = i\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[maxindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx-simplifier in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: onnxruntime>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.0.0)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (1.5.0)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from onnx-simplifier) (3.9.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (3.7.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (1.17.0)\n",
      "Requirement already satisfied: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx->onnx-simplifier) (3.7.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.7.0->onnx-simplifier) (41.0.1)\n",
      "Simplifying...\n",
      "Ok!\n",
      "[[ 5.898687   0.8682228 -3.2367797 -2.6263351  2.7641833 -0.591359\n",
      "   1.6326842]]\n",
      "     0.938\n",
      "     0.006\n",
      "     0.000\n",
      "     0.000\n",
      "     0.041\n",
      "     0.001\n",
      "     0.013\n",
      "The Expression is Angry\n"
     ]
    }
   ],
   "source": [
    "# onnx模型简单化\n",
    "!pip install onnx-simplifier\n",
    "!python -m onnxsim \"CK+_VGG19_privateTest.onnx\" \"CK+_VGG19_privateTest_sim.onnx\"\n",
    "\n",
    "#验证模型\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import onnx\n",
    "\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"CK+_VGG19_privateTest_sim.onnx\")\n",
    "\n",
    "# Check that the IR is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "onnx.helper.printable_graph(model.graph)\n",
    "\n",
    "\n",
    "import caffe2.python.onnx.backend as backend\n",
    "import numpy as np\n",
    "\n",
    "rep = backend.prepare(model) # or \"CPU\"\n",
    "# For the Caffe2 backend:\n",
    "#     rep.predict_net is the Caffe2 protobuf for the network\n",
    "#     rep.workspace is the Caffe2 workspace for the network\n",
    "#       (see the class caffe2.python.onnx.backend.Workspace)\n",
    "\n",
    "#input\n",
    "\n",
    "img = io.imread('images/anger.png')\n",
    "img = img[:, :, np.newaxis]\n",
    "img = np.concatenate((img, img, img), axis=2)\n",
    "img = Image.fromarray(img)\n",
    "inputs = transform_test(img)\n",
    "\n",
    "c, h, w = np.shape(inputs)\n",
    "inputs = inputs.view(-1,c, h, w)\n",
    "\n",
    "outputs = rep.run(inputs.numpy().astype(np.float32))\n",
    "# To run networks with more than one input, pass a tuple\n",
    "# rather than a single numpy ndarray.\n",
    "print(outputs[0])\n",
    "\n",
    "torch_data = torch.from_numpy(outputs[0])\n",
    "score = F.softmax(torch_data,1)\n",
    "max = score[0][0]\n",
    "maxindex = 0\n",
    "for i in range(7):\n",
    "  print('%10.3f' % score[0][i])\n",
    "  if(score[0][i] > max):\n",
    "        max = score[0][i]\n",
    "        maxindex = i\n",
    "\n",
    "print(\"The Expression is %s\" %str(class_names[maxindex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: onnx-coreml in /usr/local/lib/python3.6/dist-packages (1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.17.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4.1)\n",
      "Requirement already satisfied, skipping upgrade: coremltools==3.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: onnx==1.5.0 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: typing>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from onnx-coreml) (3.7.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from coremltools==3.0->onnx-coreml) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from coremltools==3.0->onnx-coreml) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->coremltools==3.0->onnx-coreml) (41.0.1)\n",
      "1/40: Converting Node Type Conv\n",
      "2/40: Converting Node Type Relu\n",
      "3/40: Converting Node Type Conv\n",
      "4/40: Converting Node Type Relu\n",
      "5/40: Converting Node Type MaxPool\n",
      "6/40: Converting Node Type Conv\n",
      "7/40: Converting Node Type Relu\n",
      "8/40: Converting Node Type Conv\n",
      "9/40: Converting Node Type Relu\n",
      "10/40: Converting Node Type MaxPool\n",
      "11/40: Converting Node Type Conv\n",
      "12/40: Converting Node Type Relu\n",
      "13/40: Converting Node Type Conv\n",
      "14/40: Converting Node Type Relu\n",
      "15/40: Converting Node Type Conv\n",
      "16/40: Converting Node Type Relu\n",
      "17/40: Converting Node Type Conv\n",
      "18/40: Converting Node Type Relu\n",
      "19/40: Converting Node Type MaxPool\n",
      "20/40: Converting Node Type Conv\n",
      "21/40: Converting Node Type Relu\n",
      "22/40: Converting Node Type Conv\n",
      "23/40: Converting Node Type Relu\n",
      "24/40: Converting Node Type Conv\n",
      "25/40: Converting Node Type Relu\n",
      "26/40: Converting Node Type Conv\n",
      "27/40: Converting Node Type Relu\n",
      "28/40: Converting Node Type MaxPool\n",
      "29/40: Converting Node Type Conv\n",
      "30/40: Converting Node Type Relu\n",
      "31/40: Converting Node Type Conv\n",
      "32/40: Converting Node Type Relu\n",
      "33/40: Converting Node Type Conv\n",
      "34/40: Converting Node Type Relu\n",
      "35/40: Converting Node Type Conv\n",
      "36/40: Converting Node Type Relu\n",
      "37/40: Converting Node Type MaxPool\n",
      "38/40: Converting Node Type AveragePool\n",
      "39/40: Converting Node Type Reshape\n",
      "40/40: Converting Node Type Gemm\n",
      "Translation to CoreML spec completed. Now compiling the CoreML model.\n",
      "Model Compilation done.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U onnx-coreml\n",
    "#转成corml模型\n",
    "import onnx;\n",
    "from onnx_coreml import convert\n",
    "\n",
    "onnx_model = onnx.load(\"CK+_VGG19_privateTest_sim.onnx\")\n",
    "cml_model= convert(onnx_model,image_input_names='data',target_ios='13')\n",
    "cml_model.save(\"CK+_VGG19_privateTest_sim_13.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
